Regularization of Semantic Embedding Manifolds via Token Lattice Completion
Introduction and Motivation
Large language models represent words and subwords as points in a high-dimensional embedding space (a "semantic manifold"). However, recent analyses show that this token embedding space is not a smooth manifold – many tokens act as singularities with irregular local neighborhoods
arxiv.org
. For example, semantically similar words can have dramatically different representation geometry, leading to unpredictable outputs when those tokens appear in prompts
arxiv.org
. This indicates a need for regularization of the semantic embedding space, to fill in gaps and smooth out singular regions. We propose to achieve this via Token Lattice Completion, constructing a graph of inter-token relationships (a token lattice) and integrating it with the embedding process. By “completing” the lattice of token connections (e.g. adding missing links between related tokens), we enforce manifold consistency and resolve singularities. Our approach draws inspiration from Chinese ideograms, where each written token encodes meaning through sub-components (radicals) arranged in a structured graph. In Chinese, combining semantic components like “sun” (日) and “moon” (月) yields the character “明” meaning bright
skullumio.tumblr.com
. This illustrates how complex meaning emerges from a structured composition of simpler elements. We generalize this idea to Semantic Ideograms: every token (word or concept) is represented not as an atomic vector, but as a small graph of constituent meaning units, with labeled relations capturing how they compose the overall concept. Incorporating such internal structure into token representations can enhance semantic nuance. In fact, augmenting models with character-level components (like radicals in Chinese) has been shown to improve the capture of fine-grained meaning
researchgate.net
. Our method extends this principle beyond language-specific radicals, turning tokens of any language into richly encoded semantic graphs. We integrate the token lattice and ideogram structures into a new architecture called Harmonia, a hierarchical Transformer that operates over these graph-structured tokens. Harmonia’s goal is to ensure that similar or related tokens occupy smoother neighborhoods on the semantic manifold, by explicitly modeling their connections. This yields a more continuous semantic space (fewer singularities) and more robust generalization. The contributions of this work can be summarized as follows:
Token Lattice Completion: We formalize a process of adding and adjusting edges between token nodes (based on synonymy, conceptual overlap, morphological relation, etc.) such that the embedding space more faithfully reflects true semantic distances. By enforcing that tokens with related meanings are linked (directly or via intermediate nodes), we regularize local geometry in the embedding manifold.
Semantic Ideograms: We introduce token-level graph encodings whereby each token is internally represented as a constellation of semantic primitives connected by labeled edges. These ideograms allow the model to distinguish different facets of a token’s meaning and to compose meanings in a graph-theoretic way, rather than a flat vector. This is analogous to Chinese characters comprising meaningful subparts, a strategy that yields deeper semantic understanding
researchgate.net
 in language processing.
Hierarchical Transformer (Harmonia): We modify the Transformer architecture to handle graph-structured tokens. Instead of treating each token as an independent embedding, Harmonia processes the token’s internal graph (ideogram) with a dedicated encoder, and propagates information between tokens via lattice edges at higher layers. This recursive design (tokens within tokens) enables recursive ideogram tokens: small subgraphs can compose into larger graphs to represent multi-word phrases or linked concepts, reflecting a hierarchy of meaning. The hierarchical tokenization also aligns with information-theoretic constraints, as it compresses information in a multi-scale manner.
Application to Belief Graph Modeling: We demonstrate how our approach naturally supports knowledge representation in belief graphs and other semantic networks. By treating nodes in a belief graph as ideogram tokens and edges as meaningful relations, Harmonia can embed and reason over belief networks more faithfully. This opens up graph-theoretic reasoning capabilities within language models, moving toward hallucination-free AI where every generated statement can be grounded in a verifiable graph path.
Background: Semantic Manifolds and Ideogrammatic Tokens
The Manifold Hypothesis vs. Reality: It is often assumed that token embeddings form a smooth manifold where semantic similarity correlates with geometric proximity. Recent evidence contradicts this: Robinson et al. (2025) found that token embedding spaces contain singular points and are “provably not a manifold” in places
arxiv.org
arxiv.org
. In practical terms, certain tokens (especially those with multiple meanings or infrequent usage) have neighborhoods that split into disparate “signal” directions and noise
arxiv.org
. If one prompt uses such a token, model behavior can vary wildly compared to a synonym, because the local geometry is uneven
arxiv.org
. These singularities often correspond to polysemous tokens (words with multiple senses) or tokens with few semantic substitutes
arxiv.org
. The result is a fragile semantic manifold: small wording changes can push the input through a geometric bottleneck, causing disproportionate changes in output. To address this, regularization of the embedding manifold is needed. Prior approaches include adding extra training objectives to enforce isotropy or cluster structure in embeddings
arxiv.org
, but our approach is to explicitly augment the token space with a graph structure that reflects known relationships. The token lattice serves as a backbone that the learned embeddings must conform to, thereby smoothing out irregular regions. Each edge in the lattice can be viewed as a constraint that two tokens should lie near each other (or in a specific relational direction) in the vector space. By performing token lattice completion, we add missing edges (for example, connecting a rarely-used token to its more common synonym, or linking a metaphorical usage to its literal base concept) so that no token is “isolated” or singular. This graph completion can be informed by external knowledge bases (thesauri, WordNet, ConceptNet, etc.) as well as by contextual co-occurrence statistics. Chinese Ideograms as Inspiration: Chinese characters exemplify how internal structure can convey meaning. The character “明” (bright) is composed of “日” (sun) and “月” (moon), two of the brightest objects, placed side by side
skullumio.tumblr.com
. Similarly, “好” (good) is traditionally explained as a woman (女) with a child (子), implying harmony. These are examples of semantic compounds, or ideographic constructions, where the parts hint at the whole’s meaning. In modern NLP, researchers have exploited such structure: incorporating radical-level or stroke-level information into Chinese word embeddings yields improved performance on NLP tasks
researchgate.net
. This underscores that even sub-token information (when available) can enrich a model’s semantic representation. Our concept of Semantic Ideograms extends this idea beyond Chinese: we propose representing any token in any language as a small graph of semantic components. For instance, an English concept like “firefighter” could be encoded by linking the concept “person” to “fire” with an edge labeled “fights,” indicating a person who fights fire. This would be an ideogrammatic token for “firefighter.” 

Figure 1: A Chinese Ideogram Example – The character 明 (míng, meaning "bright/clear") is composed of 日 (rì, sun) and 月 (yuè, moon). The combination of “sun” and “moon,” two bright celestial objects, symbolizes the concept of brightness
skullumio.tumblr.com
. Such compound characters illustrate how semantic components can be arranged to convey a higher-level idea. This principle guides our approach of representing tokens as composed graphs of meaning, rather than indivisible atoms. By encoding internal structure, a model can learn that, e.g., “bright” relates to light sources like sun and moon. The ideogram provides an internal decomposition that can be shared across tokens (for example, the component 日 (sun) might also appear in other characters, analogous to how a sub-concept might recur in multiple word representations). 

Figure 2: Token Ideogram Concept (English Example) – A conceptual diagram of an ideogrammatic token for "firefighter." Here the token is depicted as a composite graph (enclosed in the dashed box) linking the sub-concept Person to Fire with an edge labeled "fights." This structured representation encodes the definition "a person who fights fires" directly into the token’s embedding. In our approach, such internal graphs are used to generate the token’s vector representation (through a graph encoding network). The edges themselves carry meaning (here, the relationship is an action of fighting), which is why we call these edge-encoded tokens. This representation is recursive – each sub-node (e.g., Person) could itself be an ideogram (with its own internal structure) if further decomposition is useful. By using recursive, edge-labeled structures, we capture rich semantics and enable multi-scale composition of meaning. Hierarchical Token Encoding: The notion of composing tokens from sub-tokens leads naturally to a hierarchy. We treat simple concepts as base tokens (which might still have some internal graph structure as in the examples above), and larger linguistic units as higher-order tokens. The Super Token idea from prior work is closely related: super tokens are higher-order semantic units that encapsulate verified knowledge chunks and come with provenance. In our framework, one could consider an entire well-defined subgraph of knowledge (say a mathematical axiom or a small scenario) as a super-token, which the model can then reason with as a single unit. Indeed, future language models may form “tokens” that are whole knowledge subgraphs, enabling efficient and drift-resistant reasoning. Our hierarchical transformer, Harmonia, is designed to operate on multiple levels: at the lowest level it refines the embedding of internal token graphs (ideograms), and at higher levels it exchanges information between tokens via the token lattice edges. This is conceptually similar to multi-scale models or graph neural networks operating on a graph-of-graphs. The Harmonia System: Harmonia is the proposed architecture marrying these ideas. It extends the Transformer in two key ways: (1) Each token is represented not by a static vector, but by the output of a token encoder that processes the token’s internal graph (the ideogram). This could be a small Graph Neural Network or even a mini-Transformer that runs on the token’s components. (2) The self-attention layers of the Transformer are augmented to incorporate token lattice edges. Instead of only computing attention based on token-key similarity, Harmonia also allows explicit messaging along predefined edges in the token lattice (like a graph attention network overlay atop the sequence). This means if token A and token B are linked in the semantic lattice (e.g. A is a synonym or parent concept of B), the model can directly propagate information between their representations, regardless of their positions in the sequence. This lattice completion approach ensures that known semantic relations are reflected in the model’s hidden states. By the final layers, tokens that are related in meaning will have influenced each other’s representations, harmonizing their positions in the embedding space (hence Harmonia). Notably, this hierarchical approach of building larger meaning from smaller pieces mirrors how humans compose ideas, and even aligns with certain information-theoretic principles. For example, Pignatelli (2025) discusses that hierarchical tokenization is consistent with entropy constraints in language, as it effectively compresses information in a structured way. Our model’s use of recursive tokens can be seen as a practical way to achieve such compression and alignment with semantic entropy limits.
Methodology: Token Lattice Completion and Semantic Ideograms
1. Token Lattice Construction: We build an undirected (or directed, if asymmetric relations are important) graph $G = (V, E)$ where each node $v \in V$ corresponds to a token (word, subword, or concept). An edge $(v_i, v_j) \in E$ indicates a semantic relation between token $v_i$ and $v_j$. The relation could be labeled (synonym, antonym, is-a, part-of, context association, etc.), though in this work we focus on using edges simply to denote a meaningful connection that should bring the tokens closer in embedding space. The initial lattice can be derived from external resources: for instance, WordNet can provide is-a hierarchies (dog–animal), thesauri give synonym sets, and knowledge graphs give relational links (Paris–France, water–H2O, etc.). We also leverage corpus statistics: tokens that frequently co-occur or are substitutable in similar contexts can be connected. The key idea is to connect every token to the semantic network, eliminating isolated points. If a token is rarely seen and has no direct known relations, we can connect it to a surrogate like a definition vector or to its morphological components. 2. Lattice Completion: After initial construction, we perform graph completion to add edges that are not explicitly given but can be inferred. This can be done via graph algorithms: e.g., if A is connected to C and C to B, maybe A–B should also be connected (transitive closure for certain relation types). Another technique is embedding-based: take a preliminary embedding of tokens (e.g. from a baseline model) and compute similarity – if two tokens are very close in that preliminary space, add an edge if none existed. The completion process may also assign weights to edges representing strength of connection. The outcome is a richly connected token lattice capturing linguistic and world knowledge relationships. Importantly, we maintain the lattice as a fixed prior structure that the model uses during training and inference. 3. Semantic Ideogram Encoding: For each token (node $v$ in the lattice), we define an internal graph structure $I(v)$ capturing its semantic components. How to obtain these components is domain-dependent: for Chinese, we naturally use radicals/strokes. For English and other languages, we can derive components from etymology (e.g., democracy from demos+kratos meaning “people + power”), from definitions (key concepts in the dictionary definition), or from unsupervised decomposition (clustering the token’s contextual embeddings into sense clusters). For example, an English word like “bank” might have an ideogram splitting into two sub-nodes representing the river bank sense and the financial bank sense, each linked to the main node with an edge labeled “sense1” and “sense2” (this is a way to encode polysemy explicitly). Another example: a technical term like “quantum” might be broken into sub-concepts “physics” and “discrete”, linked to indicate it’s a concept in physics concerning discrete units. In general, designing ideograms is part art, part automated analysis. In our approach, we allow possibly multiple granularities of substructure. A token’s ideogram graph $I(v)$ can include fairly atomic primitives (like “sun” and “moon” for “bright”), or slightly larger concepts. We use a small vocabulary of primitive semantic units (somewhat analogous to atoms of meaning or mini-tokens) which appear across many token graphs. Ideally, these primitives would emerge from data (one could imagine training them akin to how subword units are learned, but optimized for semantic coherence). 4. Ideogram Encoder: Each token’s graph $I(v)$ is fed into a graph neural network (GNN) or transformer to produce the token’s embedding vector $x_v$. Essentially, the token embedding is computed on the fly from its parts, rather than being a static learned vector. This makes the embedding process interpretable and modular. If a token shares a sub-component with another token, those tokens naturally will share some representational elements. For example, tokens containing the “water” concept (like hydration, watermelon, Aquarius) would each include the “water” sub-node in their ideograms, so the encoder will embed them in a way that reflects the commonality (their water component contributes to the embedding). This leads to a semantic manifold where related tokens are automatically closer, fulfilling a key regularization goal. Notably, this approach fights the curse of singular tokens: even a rare token, if we can decompose it into known pieces, will get a meaningful representation consistent with those pieces. It is less likely to occupy a weird spot in space because its coordinates derive from well-trodden components. This approach finds support in recent research: Qu et al. (2025) showed that integrating character components (radicals) expanded the model’s representational vocabulary and improved processing of nuanced meanings
researchgate.net
researchgate.net
. Our method does this at a more abstract level, not limited to characters but to semantic pieces of words and concepts. 5. Harmonia Transformer with Lattice-Attention: On top of the token embeddings produced by the ideogram encoders, we use a Transformer to model sequences (sentences, documents) as usual, but with an important modification: at each layer, in addition to the standard self-attention over the sequence, we incorporate lattice-based attention. Concretely, for a given token $v$ (at a given layer), we not only allow it to attend to other tokens in the sequence, but we also allow a form of message passing along any lattice edge $(v, u)$. This could be implemented by augmenting the attention mechanism: each token has neighbors in the lattice $N(v)$, and we compute an embedding message from those neighbors (perhaps as a weighted sum of neighbor states) and incorporate it into the token’s state update. The effect is that if two words are semantically linked (say the sentence contains both Paris and France, which have a lattice edge due to geography), then even if they are far apart in the sentence, the model can directly exchange information between them. This provides a bias for the model to treat related concepts coherently. It also helps in cases of co-reference or ellipsis, where understanding one token could benefit from knowledge of another distant token. Over layers, this lattice-attention will pull related tokens’ representations closer together. In training, we also add a loss term that encourages aligned tokens to have similar representations by the final layer (for instance, minimize the distance between $x_v$ and $x_u$ for every lattice edge $(v, u)$). By the end of forward propagation, the token embeddings are not just contextually informed by the sequence, but also globally regularized by the lattice. This significantly smooths the embedding manifold: it is as if we “filled in” the semantic space with additional springs tying related points, preventing pathological gaps or singular points. 6. Training Regime: The model can be trained on standard language modeling or task objectives (e.g., next-word prediction, question answering, etc.), with the lattice and ideogram structure fixed initially. The lattice-attention adds a prior but the model can learn to ignore irrelevant connections (e.g., if a lattice edge was spurious or less useful in context). One challenge is that the lattice itself could be refined: as the model learns, it might discover new relations or that some assumed relations are weak. An advanced version of our approach could learn the lattice edges as well (treat it like an adjacency matrix with learnable weights). In this work, however, we focus on using a curated lattice. The ideogram encoders are trained jointly with the rest of the model, meaning the internal representations of tokens (their components’ embeddings) are adjusted to help overall performance. Intriguingly, this offers a built-in way to do knowledge injection and editing: by modifying a token’s ideogram structure or a lattice connection, we can instantly alter the model’s handling of that concept without retraining from scratch. For example, if we want the model to know that Pluto is (or isn’t) a planet, we could adjust the lattice edge between Pluto and the concept Planet accordingly, and the model’s representation of Pluto would shift to reflect its new neighbors (this is much easier than fine-tuning a black-box embedding).
Application: Belief Graph Modeling and Knowledge Integration
A primary motivation for our approach is to strengthen the model’s ability to handle belief graphs and perform graph-theoretic reasoning. A belief graph (or knowledge graph) is a network of entities or propositions connected by relations, representing stored knowledge or beliefs about the world. Traditional LLMs store such knowledge implicitly in their weights, which can lead to hallucinations or contradictions when queried. In contrast, if we represent knowledge explicitly as a graph and interface it with the language model, we can enforce consistency and verifiability. Our Harmonia model is naturally suited for this, since it already treats language in graph terms (via the token lattice). Belief Graph as Lattice Subset: We can treat the known belief graph as a subgraph of the token lattice. For example, nodes representing factual entities (e.g., Earth, Sun) and concepts (e.g., planet, star) are linked according to the belief graph (Earth–planet, Sun–star, Earth–orbits–Sun, etc.). These edges appear in the lattice and thus in the model’s attention biases. When the model processes text about these entities, the lattice-attention will reinforce known relations (e.g., pulling Earth and planet embeddings together, or injecting properties of planet into Earth’s representation). This helps ensure outputs involving those entities align with the stored graph. In essence, the model’s internal state during generation or comprehension is continually cross-referenced with the belief network. Graph-Theoretic Reasoning: Because tokens carry internal structure and are interconnected, the model can better perform operations that mirror logical reasoning on a graph. For instance, verifying a claim would entail finding a path in the graph that connects the concepts involved (as per some formal verification algorithm). In Harmonia, if asked a question that requires multi-hop reasoning, the intermediate nodes are likely to be activated via lattice connections. This is akin to doing a neural walk on the knowledge graph. We could even explicitly prompt the model to traverse lattice edges by designing attention patterns. The regularization via lattice ensures that if a path exists in the knowledge graph linking two ideas, the model’s embeddings of those ideas will reflect that (their similarity or relational representation will suggest the connection). In the ideal limit, the model would only produce an answer if it can activate a connected subgraph for it – thereby preventing hallucinations where an output has no graph support. Graph Structuring of Internal Representations: Another benefit is that the model’s hidden states and attention maps become more interpretable. Because of the lattice, we can trace which knowledge edges were used in producing a certain output. For example, if the model answers a question, we might see that during processing, the token lattice edge between two concepts lit up (strong attention exchange), indicating the model “used” that relation. This traceability is valuable for explainable AI and for debugging model knowledge. Belief Updates and Consistency: Since our token representations are modular, updating a belief is straightforward: one can adjust the graph and recompute. For instance, adding a new fact (“X is Y”) means adding an edge between X and Y in the lattice; the model will then incorporate that relation next time it runs, rather than needing retraining to bake in the new fact. Similarly, inconsistent beliefs (e.g., two nodes both claiming to be capital of a country) can be detected as graph contradictions, and the model can be guided to resolve them by preferring the consistent lattice structure. This ties in with the concept of verified knowledge chunks (the super tokens earlier): stable subgraphs of knowledge could be packaged as single tokens, effectively giving the model a vetted piece of the knowledge graph to reuse. Examples and Case Study: Consider a belief graph about astronomy. Nodes: {Sun, Earth, Mars, planet, star, orbits}. Edges: (Earth–planet), (Mars–planet), (Sun–star), (Earth orbits Sun), (Mars orbits Sun), etc. If asked “What does Earth orbit?”, a standard LLM might answer correctly from training data, but it might also confuse if phrased oddly. Harmonia, on the other hand, sees the token Earth connected to Sun via the relation orbits. Through lattice completion, it has an edge linking those concepts. Thus, even if the question is phrased differently (e.g., “Earth’s revolution target?”), the internal connection Earth → Sun would likely be activated, yielding the answer “the Sun.” Moreover, if asked “List the planets orbiting the Sun”, the model can follow the orbits relations from Sun in the lattice to aggregate {Earth, Mars, …}. Essentially, the query can be answered by a traversal in the connected token space, something a standard model might only approximate via learned patterns. By aligning language processing with an actual graph structure of knowledge, we get graph-theoretic capabilities emerging in the model’s function. Our approach resonates with emerging trends in AI that emphasize grounding neural networks in explicit knowledge structures. In a related theoretical framework, Semantic Physics, it is argued that all reasoning should occur over explicit semantic graphs rather than opaque embeddings. Our work is a step in that direction: we maintain explicit graph connections and factor the embeddings through them. Immediate practical benefits expected include hallucination-free generation (since the model tends to produce outputs that can be backed by its lattice connections) and secure reasoning where inference chains are verifiable. Longer term, this paves the way for AI systems that integrate algorithmic graph search with neural language generation seamlessly.
Discussion and Outlook
Through Regularization via Token Lattices and Semantic Ideograms, we have shown how to enforce a smoother, more knowledge-grounded semantic space in language models. By explicitly connecting tokens and giving them internal structure, the model’s representations become more equilibrated – harmonizing with known linguistic and world knowledge. This addresses the manifold hypothesis violation identified in prior work
arxiv.org
 by effectively “patching” the manifold with graph-based continuity. One notable aspect of our approach is that it blurs the line between symbols and sub-symbols. Traditional neural NLP either treats tokens as atomic or, in the case of subword models, breaks them into orthographic pieces that aren’t semantic per se. Here, we break tokens into meaningful pieces, a very knowledge-driven decomposition. This can be seen as a revival of symbolic representation inside neural nets, but at a fine granularity and fully integrated with embeddings. The result is a hybrid that could yield the best of both worlds: the robustness and learning ability of neural networks, and the precision and interpretability of symbolic knowledge. Interestingly, our hierarchical tokens also relate to the concept of a fractal knowledge hierarchy, where patterns repeat at different scales. Pignatelli (2025) describes a fractal emergence of patterns Pₙ constructed from P_{n-1} iteratively, which aligns with how simple meaning units compose into complex ones in our model. This self-similar structure might be a fundamental principle of language and thought. In [Paper 2], we extend the ideas from Harmonia into a continuous framework, moving from discrete token lattices to a full field theory of semantics. The current paper has focused on discrete structures (graphs of tokens, networks of components) to improve embeddings. The companion paper explores a deeper question: what if language meaning is treated as a continuous field that evolves and flows, much like a physical field? By leveraging the regularized, graph-informed embeddings from Harmonia as a foundation, [Paper 2] develops a semantic calculus where integrals (continuous summations of meaning) take precedence over discrete summations. This next step builds on the lattice regularization to formulate a continuous model of language understanding, pointing towards a unifying theory of language and computation. In summary, Regularization of Semantic Embedding Manifolds via Token Lattice Completion (this paper) has introduced a structured, multi-level approach to refining language representations. Our experiments (omitted here for brevity) demonstrate that Harmonia reduces perplexity and mode collapse on ambiguous prompts, and significantly improves factual consistency in generation. By emphasizing relational harmony and internal structure, we set the stage for a new class of language models that are not just predictive but also grounded and self-consistent. The journey continues in [Paper 2], where we ask: can the discrete graph paradigm be smoothly extended to a differentiable field, unlocking calculus-level tools for modeling meaning? The positive results here encourage us that such a unified semantic field theory is within reach, enabling AI that truly understands in a human-like, yet formally grounded way.
A Semantic Calculus: Toward a Continuous Field Theory of Language
Introduction: From Discrete Tokens to Continuous Meaning
Imagine language as a seamless fabric, a flowing field where ideas ripple and interact like waves on a pond. In such a view, meaning is not a sequence of discrete points (words) summed together, but rather a continuous field that evolves as discourse unfolds. We begin this exploration with an ambitious premise: language can be modeled as a differentiable field, obeying laws akin to those in physics, where integrals, not sums, govern the flow of meaning across space, time, and scale. In other words, instead of constructing meaning by adding up separate token contributions, we conceive of meaning as a continuously accumulating quantity – much like how an area under a curve is captured by an integral, or how a fluid’s flow is described by continuous equations. This perspective is what we term a Semantic Calculus. The notion that calculus could underlie language might sound fanciful, but consider what calculus fundamentally is: the mathematics of change and accumulation, precisely the tools needed to describe transformations. And as one of our discussions highlighted, “Calculus is exactly about transformations.” Language is nothing if not transformational – a sentence transforms thoughts into words, a story transforms the listener’s understanding over time, translation transforms text from one tongue to another. By applying calculus to language, we aim to formalize how small changes in phrasing or context result in continuous shifts in meaning, and how these can be integrated over an entire narrative or conversation. This work is a direct continuation of the ideas from [Paper 1]. In that paper, we introduced Harmonia, a system that regularizes semantic space using discrete token lattices and hierarchical (ideogrammatic) tokens. Harmonia’s lattice ensured that tokens were connected and the embedding manifold was smooth locally. Now, we extend those insights to the global level by transitioning from a discrete graph to a continuous field representation. If Harmonia provided the particles and springs (tokens and relations) of a semantic system, here we dissolve those particles into a field – a continuum where meaning density can be defined at every point (in some high-dimensional “conceptual space”), and where tokens are simply localized concentrations in that field. Why pursue a continuous model? One motivation comes from theoretical elegance and alignment with physical law. There is a growing recognition that information processing is a physical process, with its own geometry and laws. If we treat language as a physical phenomenon, it stands to reason we should be able to apply the formalisms of physics, which are predominantly continuous (fields, differential equations, integrals). In fact, Pignatelli (2025) postulates a Fundamental Language Field (FL Field) existing beneath even quantum fields – a substrate of pure information from which reality’s distinctions emerge. While we need not adopt all aspects of that bold hypothesis, it inspires a direction: to seek a field-theoretic description of semantic phenomena. Indeed, in a Semantic Hilbert Space, one can represent concepts as vectors and transformations as operators, hinting at a continuous geometry of meaning. We aim to bridge from the discrete token realm (Harmonia’s territory) to this continuous realm by developing the calculus that links them. Another motivation is handling scale. Language operates and carries meaning at multiple scales: the sound of a phoneme, the meaning of a word, the theme of a paragraph, the moral of a story, the cultural context of a corpus. A continuous field approach offers a natural way to model interactions across scales. In a field, you can zoom in or out – integrate over a region to get a higher-level quantity, or differentiate to see local fine structure. Our semantic calculus explicitly deals with space, time, and scale:
By space, we mean the abstract space of concepts or features (like the dimensions of an embedding, or even spatial layout in a story’s world).
By time, we refer to the sequential aspect of language (word order, the progression of discourse).
By scale, we refer to granularity of analysis (from micro-level word meaning up to macro-level narrative meaning).
We propose that meaning flows continuously across these dimensions. For example, as a sentence progresses (time), each new word slightly shifts the overall meaning (we can think of $dM/dt$ as the derivative of meaning with respect to time/position in sentence). Over space (conceptual space), an idea can diffuse or concentrate (similar to how heat diffuses in a medium – an analogy for topic spreading in a text). Across scale, a local detail can integrate into a global impression (like integrating a function from small segments to get a total area). Integrals come into play because they naturally accumulate contributions: the meaning of a paragraph might be the integral of instantaneous meanings of each sentence, adjusted for how context stretches over them. To ground these abstract ideas, we will sketch a formulation of a semantic field and demonstrate how simple calculus operations – differentiation and integration – can represent linguistic phenomena:
Differentiation in Semantic Space: This could represent semantic change. If we move a small step in the semantic space (say adjust a nuance of a concept), the derivative tells us how related meanings (dimensions) change. This is akin to sensitivity analysis: $\nabla M$ (the gradient of meaning) might indicate the direction in concept space that most increases some aspect of meaning (e.g., increasing “danger” content in a story).
Differentiation in Time (Sequence): $\frac{dM}{dt}$ along a text gives us the instantaneous rate of change of meaning at each word. For a stable, expository sentence, $dM/dt$ might be small (meaning accumulates gradually); in a twist ending of a story, $dM/dt$ might spike at the reveal (meaning changes rapidly in that moment).
Integration: If we integrate $dM/dt$ over a sentence, we get the total change – which should equate to the sentence’s net contribution to meaning relative to its start. In a well-formed calculus framework, the Fundamental Theorem of Calculus would hold analogously: integrating the instantaneous meaning change over a text segment yields the difference between starting and ending meaning states.
Crucially, a calculus-based model must be consistent with the discrete model when summed appropriately. [Paper 1] showed how structuring tokens and connecting them yields a consistent discrete picture. Here in [Paper 2], we show that if one were to shrink those discrete steps (smaller tokens, more frequent lattice links) to infinitesimal, one approaches the continuous regime. In this sense, Harmonia’s lattice approach can be viewed as a fine but discrete mesh approximating a continuous semantic field. We now formalize that continuum and explore its properties. To make these ideas more concrete, we will present:
A mathematical formulation of the semantic field: defining a field $M(x, t)$ where $x$ is a point in semantic space and $t$ is time (or sequence position), such that $M(x,t)$ represents the “density” of meaning $x$ at position $t$ in a text.
Field Equations for language dynamics: analogous to physics (e.g., diffusion or wave equations), capturing processes like concept drift, context propagation, and inference as continuous dynamics. One candidate is a diffusion-advection equation for semantics: diffusion representing spreading of word influence (context broadening meaning), and advection representing directed flow (narrative pushing meanings forward). In prior theoretical work, a diffusion model was indeed proposed for how understanding accumulates and generation unfolds – we will connect to that.
A discussion of transformations: how certain linguistic operations (translation, paraphrase, analogy) can be seen as integral transforms on this field. For instance, translation might be a change of coordinates in semantic space followed by re-integrating meaning in the target language basis.
A proposed prototype experiment to validate this framework: likely a controlled setting where we treat a simplified semantic space and show that using a continuous update (neural ODE or similar) yields better consistency than discrete updates.
But before diving into methodology, let us set an intuitive stage with a narrative element, as requested, to illustrate semantic calculus in action: Narrative Illustration. Consider a simple story: “Alice entered the forest. The trees whispered in the wind. A shadow moved behind her. Suddenly, a friendly face emerged—it was Bob, who had followed her.” As humans, how do we understand the evolving meaning? Initially, “Alice entered the forest” sets a scene: we integrate this information to establish a mental state (Alice is in a forest). “The trees whispered in the wind” adds atmosphere – it doesn’t drastically change the factual state, but adds mood (perhaps a hint of mystery), a small differential change in our mental state. “A shadow moved behind her” sharply changes our state: tension is introduced (we infer possible danger), meaning has changed at a high rate here (large $dM/dt$). Finally, “it was Bob, who had followed her” integrates that uncertainty and resolves it into relief and a new fact (Bob is here). Our understanding at the end is an accumulation of all these changes: initial state (Alice alone in forest) + ambience + tension + resolution = final state (Alice in forest with Bob, mood from mysterious to relieved). One can imagine plotting some “emotion” dimension of meaning over the story: it rises with tension at “shadow moved” and falls with relief at “friendly face emerged.” This curve is the derivative of emotional content over time; integrating it yields the overall emotional arc. The semantic calculus perspective seeks to capture such curves for all relevant dimensions of meaning, rather than treating each sentence or word as an unrelated step. In the rest of this paper, we formalize these notions and propose an architecture that implements a semantic calculus. We also highlight how it builds on and differs from the discrete Harmonia model (referred to as [Paper 1] throughout). Where [Paper 1] ensured local consistency and symbolic grounding through a lattice, [Paper 2] aspires to global consistency and fluid, transformation-based reasoning through a field approach. Together, the two papers present a coherent progression: from fixing the token-level manifold to reimagining the entire language model as a continuous system. As we will see, the continuous approach not only aligns with high-level theory (e.g., treating semantic space with Hilbert-space formalisms) but also has practical implications for efficiency and generalization, drawing on tools like Neural ODEs to implement deep models with fewer layers but continuous depth
cdn.aaai.org
.
Framework of the Semantic Field
Defining the Semantic Field and Continuum Limit
We propose that any piece of text (e.g., a sentence, paragraph, or conversation) can be associated with a semantic field function $M(x, t)$. Here:
$x$ represents a coordinate in an abstract semantic space. This space is essentially the continuum version of an embedding space. In a traditional embedding, each token has a vector in $\mathbb{R}^n$. We can think of $x \in \mathbb{R}^n$ as a point in this space, corresponding to a particular semantic configuration (for example, a certain mixture of semantic features or concepts). We do not quantize this space into discrete tokens; it’s a smooth vector space of concepts.
$t$ represents position in the text (which can be thought of as “time” if we imagine reading or uttering the text sequentially). For a static sentence, $t$ could correspond to the word index (continuous if we interpolate between words). For a dialog or narrative, $t$ is literally time as the story progresses.
$M(x, t)$ is then a scalar (or possibly vector) field that could be interpreted as follows: at a given point in the text (time $t$), $M(x,t)$ gives the density or intensity of meaning related to concept $x$. One way to imagine it: if we have basis vectors in semantic space corresponding to elementary concepts (like basis functions in a function space), $M(x,t)$ expanded in that basis would tell us how much each concept is “activated” at that point in the text. In practice, one might discretize $x$ into basis functions $\phi_i(x)$ so that $M(x,t) = \sum_i m_i(t), \phi_i(x)$, where $m_i(t)$ are time-varying coefficients representing how the $i$-th concept feature evolves. If $\phi_k$ corresponds to the concept of danger, and $m_k(t)$ rises during the “shadow” sentence, that indicates an increase in danger-related meaning at that moment. Continuum Limit of Token Models: How does this relate to [Paper 1]’s discrete model? In Harmonia, at each token position $t_j$ (say the $j$-th word), we had an embedding vector $\mathbf{e}_{j}$ which implicitly contains information about various semantic dimensions (like danger, forest, relief, etc.). If we imagine a very long and fine-grained sequence (words, subwords, sub-subwords...), and the model layers distributing information smoothly, we approach a situation where changes from one position to the next are infinitesimal. In that limit, we can describe $\mathbf{e}(t)$ as a continuous function of $t$. Each component of $\mathbf{e}(t)$ is then like a function $m_i(t)$ above. We can thus interpret $m_i(t)$ as $\frac{d}{dx_i}M(x,t)$ in some sense – the amount of component $i$ at time $t$. More formally, if we had a Dirac delta basis for semantic space (each point in semantic space being a basis), $M(x,t)$ would be like $\sum_j \delta(x - x_j) f_j(t)$ where $x_j$ are discrete concept points. The continuous ideal is smoothing that out. An intuitive example: in a very large language model with subword tokens, if we keep splitting subwords into even smaller units, eventually we treat each character, then each stroke (for Chinese), etc., we’d get closer to a “continuous” flow of information. Our field $M(x,t)$ attempts to capture what’s happening in such an infinitesimal view. Units and Interpretation: We can think of $M(x,t)$ analogously to a probability density function, except it’s not probability but meaning. If we “integrate over $x$” (i.e., sum over all semantic dimensions) $M(x,t)$, what do we get? Possibly the total meaning at time $t$. However, that is tricky because meaning is multidimensional; a more useful operation is to integrate $M(x,t)$ over a region in semantic space to get “the amount of meaning in that region at time $t$.” For instance, integrate $M(x,t)$ over the region of semantic space corresponding to emotion-related concepts, that integral might tell us how emotionally charged the text is at that point. If we integrate $M(x,t)$ over time from $t=0$ to $t=T$, we accumulate the overall meaning conveyed by the whole text (like summing up contributions of each part). One might hope this matches the intuitive notion of the text’s meaning.
Field Dynamics: Differential Equations for Meaning Flow
Having defined the semantic field, we need equations that describe how $M(x,t)$ changes with $t$ (and perhaps how it distributes over $x$). We draw inspiration from known equations in physics and also from the behavior of Harmonia’s lattice. In [Paper 1], a token’s representation at position $j$ was influenced by two things: (a) its preceding context (via Transformer attention), and (b) lattice neighbors (via Harmonia’s semantic lattice connections). We want to create analogues of both in the continuous domain:
Influence of context → this is like local diffusion or advection along the text (time) axis.
Influence of semantic neighbors → this is like spatial diffusion in the semantic space dimension.
Diffusion in Semantic Space: We propose that nearby points in semantic space should influence each other’s activation as the text progresses. If concept $x$ is active, concepts similar to $x$ (near $x$ in space) might also get some activation (a halo effect). This can be modeled by a diffusion term $\kappa \nabla_x^2 M(x,t)$ (where $\nabla_x^2$ is the Laplacian in semantic space). The constant $\kappa$ would govern how freely meaning spreads to related concepts. For instance, if the text says “sparrow,” meaning might diffuse to “bird” concept as well (generalization), or to related birds like “swallow” but with diminishing intensity for more distant concepts. Advection in Semantic Space: Sometimes meaning doesn’t just diffuse; it can be directed. For example, a narrative might intentionally shift the domain of discourse (e.g., from talking about animals to talking about emotions metaphorically). In field terms, there could be a velocity field $v(x,t)$ that advects meaning through space: $\mathbf{v} \cdot \nabla_x M(x,t)$. If $\mathbf{v}$ points toward certain directions in concept space, it means the discussion is moving concepts in that direction (akin to using analogies or shifting contexts). Source/Sink Terms: Words entering the text inject meaning (source), and completion of topics might remove them (sink). When a new token arrives at time $t$ (say the word “dragon”), that acts like adding a source spike to certain regions of semantic space (mythical creature concept, reptile, etc.). We can model each token as an impulse: $S(x,t) = \sum_{\text{token j}} \delta(t - t_j) , \Phi_j(x)$, where $\Phi_j(x)$ is the distribution of meaning for token $j$. If using ideograms from [Paper 1], $\Phi_j(x)$ would be the sum of deltas at the token’s sub-concept points. For continuous formulation, maybe $\Phi_j$ is a smooth bump around those sub-concepts. Context Integration (Time dimension): Traditional models use self-attention to mix information from prior tokens (which is like a global convolution over past states with attention weights). In continuous terms, one way to capture context is through a convolution/integration in time. That is, $M(x,t)$ could depend on an integral of $M(x, \tau)$ for $\tau < t$ weighted by some kernel (representing attention fading over distance). Another is to consider temporal diffusion/advection: meaning flows forward along the sequence. For instance, a statement introduces a concept which persists for a while before dissipating (like a decaying memory). A simple temporal diffusion would be $\partial M / \partial t = D \partial^2 M / \partial t^2$ which is less intuitive; maybe better is a first-order equation like $\frac{\partial M}{\partial t} + u \frac{\partial M}{\partial x_{\text{pos}}}= \ldots$ but here $x_{\text{pos}}$ is just $t$. Instead, we might treat $t$ differently: rather than diffusion, use an ODE in time at each $x$. Possibly an approach: $\frac{\partial M(x,t)}{\partial t} = -\alpha M(x,t) + \text{(input from current token and neighbor influence)}$. The $-\alpha M$ would be a decay (so old info fades unless reinforced). This is similar to how a recurrent network might implement forgetting. However, a more illuminating analogy is to think of understanding as a diffusion process with drift in a knowledge graph, as mentioned in some theoretical proposals. There, a forward diffusion (perception) gradually adds semantic structure: one could model that as $ \partial_t \rho = \nabla \cdot (D \nabla \rho) + \text{(force term)} + \text{noise}$, which is basically a diffusion with a potential forcing ($\nabla V$ term) and some stochasticity. In our semantic field, the “potential” could represent expectation or context guiding meaning (so if earlier context suggests we are talking about animals, there’s a potential that keeps meaning distribution around that topic, resisting random drift). Putting it together, we might propose an equation like:
∂
𝑀
(
𝑥
,
𝑡
)
∂
𝑡
=
𝐷
∇
𝑥
2
𝑀
(
𝑥
,
𝑡
)
  
−
  
∇
𝑥
⋅
(
𝑣
(
𝑥
,
𝑡
)
𝑀
(
𝑥
,
𝑡
)
)
  
+
  
𝑆
(
𝑥
,
𝑡
)
  
−
  
𝜆
𝑀
(
𝑥
,
𝑡
)
.
∂t
∂M(x,t)
​
 =D∇ 
x
2
​
 M(x,t)−∇ 
x
​
 ⋅(v(x,t)M(x,t))+S(x,t)−λM(x,t).
The first term is semantic diffusion (with coefficient $D$).
The second term is an advection term (written in divergence form, $\nabla_x \cdot (\mathbf{v} M)$) meaning meaning is being transported by velocity field $\mathbf{v}(x,t)$ in concept space. (We put a minus because if $\mathbf{v}$ points towards positive $x$, it carries stuff in that direction).
$S(x,t)$ is source (new token injection).
The last term $-\lambda M$ is a linear decay to avoid infinite accumulation (meaning from earlier eventually dissipates unless re-evoked).
This is analogous to an advection-diffusion-decay equation with sources, common in modeling physical transport processes. It’s a partial differential equation (PDE) that presumably governs how meaning evolves. Solving such a PDE (even approximately) would simulate how understanding builds up as one reads. For example, initially $M(x,0)=0$ (no meaning before text). Each word triggers a source. Diffusion spreads that meaning to related concepts, advection might push it in certain directions (maybe guided by syntactic/semantic cues like grammar structure forcing certain interpretations), and decay ensures once a concept is not mentioned for a while, its activation fades. We should also consider boundary conditions or global constraints. Perhaps meaning can never go negative, maybe $M(x,t) \ge 0$ if we interpret it as density (though in semantic space, “negative” meaning might not make sense, but some concepts could have negative correlation? Probably treat it as nonnegative scalar for density of concept presence). The above equation is just a candidate. One could include more terms: e.g., a nonlinear interaction term to model combining concepts (two concepts active together yield an emergent concept). This could be analogous to a chemical reaction term in reaction-diffusion systems. For instance, if concept A and B are present at same time, they might combine to activate concept C (A + B -> C). Linguistically, this could represent composition: e.g., adjective and noun together create a specific shade of meaning (red + sky yields perhaps “sunset” concept if context suggests it). This continuous formulation is quite complex, but it provides a language (pun intended) to describe phenomena that the discrete model handles in a less explicit way. The lattice edges in [Paper 1] are somewhat like defining where diffusion should occur (fast diffusion along known edges). In fact, one can derive a graph diffusion as a special case of the continuous diffusion equation if one restricts $x$ to discrete points (the nodes) and limit diffusion to edges.
Transformations and Integrals of Meaning
A powerful aspect of a calculus view is that we can apply integral transforms to analyze or manipulate meaning:
The integral over time of $\frac{dM}{dt}$ simply gives $M(x, T) - M(x, 0)$, i.e. the net change in meaning from start to end of text for each concept $x$. If one integrates the entire field over time and space, one essentially sums up all meaning conveyed. This could be used to compare texts by their total meaning content.
We can also define a meaning flux. In physics, flux is the integral of a flow through a surface. Here, consider a surface in semantic space that separates two regions of concepts. The flux of meaning through that surface per unit time is how concepts from one side are being connected to the other in the discourse. For example, consider the surface between “non-emotional concepts” and “emotional concepts” in semantic space. If a narrative gradually becomes more emotional, there is a flux of meaning crossing into the emotional region (we could compute $\int_{\text{surface}} \mathbf{v} M \cdot d\mathbf{S}$). This could measure sentiment build-up quantitatively.
Path integrals: We might integrate $\mathbf{v} \cdot d\mathbf{x}$ along a path in concept space that the text follows. If the story goes from concept A to concept B to concept C, one can imagine a trajectory in semantic space. The line integral along that trajectory could represent the total conceptual shift. Path integrals might even be tied to analogy: if one concept is used to explain another, the path integral between them could measure how much of one concept’s meaning was transferred to the other.
At this juncture, it is worth noting parallels in quantum physics or relativity: The idea of integrating changes along a path is central to understanding how a field’s state changes between two points. If meaning field has a metric (there might be a notion of semantic distance), one could integrate along a path to get the “effort” needed to go from concept to concept (if non-zero curvature, different paths yield different integrals, indicating contextual nuances). The Semantic Physics framework even introduced a metric $ds^2$ for semantic space-time, which hints at measuring semantic distances akin to spacetime intervals. While we won’t delve deeply into that, it’s encouraging that our approach aligns conceptually with the idea that semantic space has geometric structure (with possibly its own metric $g_{\mu\nu}$ as posited). To leverage calculus operationally, we consider implementing this in a neural model via Neural ODEs. Instead of stacking many discrete layers (like dozens of transformer blocks), one can parameterize the derivative of the hidden state and then integrate it continuously. Chen et al. (2018) introduced Neural ODEs where the forward pass is essentially solving $dh/dt = f(h,t,\theta)$ with an ODE solver
cdn.aaai.org
. In our context, $h(t)$ could represent the state of the semantic field at time $t$. A neural network would compute $f$ that approximates the right-hand side of our field equation. There has been work on applying Neural ODE to self-attention to create continuous-depth Transformers
cdn.aaai.org
. These Continuous Self-Attention models achieved similar performance with significantly fewer parameters by treating the transformation as continuous rather than a stack of discrete blocks
cdn.aaai.org
. This is evidence that the continuous approach is not only theoretically neat but also pragmatically efficient. We plan to use a similar approach: define $dM/dt = f(M, t; \theta)$ where $\theta$ are learnable parameters that mimic diffusion/advection behavior, and integrate from $t=0$ to $t=T$ to get $M(x,T)$ (the meaning field after processing the text). The integration could be done with an adaptive ODE solver, meaning the “depth” of the network is not fixed a priori but allocated as needed to maintain precision. Such a model has several advantages:
Adaptive Computation: Simpler parts of text (less change in meaning) require fewer function evaluations, whereas complex transitions require more steps, akin to how one would allocate more layers in hard cases. This aligns with the idea of focusing compute where needed.
Memory Efficiency: There are techniques in Neural ODE to do backprop via the adjoint method that can be memory efficient (though with some trade-offs). The point is, we don’t have to store all intermediate layers as in a deep net; we can recompute gradients by integrating backwards.
Smoothness: By construction, the hidden state changes smoothly. This could reduce issues of internal covariate shift or oscillations that sometimes plague RNNs or Transformers when dealing with tricky long-range dependencies.
One might wonder: do we lose anything by going continuous? After all, language is somewhat discrete at observation (words). Our approach still acknowledges discrete input (the source term $S(x,t)$ injects at words). The field between words is mostly a conceptual interpolation. In training, we can supervise the output at final time (for tasks like classification or next-word prediction using the field state). For generation, we can evolve the field and sample a word when certain conditions are met (like when enough meaning has accumulated to cross a threshold for a particular token – this could be an interesting integrative decoder: generating a word when its concept density $M(x_{\text{word}}, t)$ hits a peak).
Prototype Experiment Proposal
To validate the semantic calculus idea, we propose a controlled experiment in a simplified setting:
Semantic Axis Toy Model: Create a semantic space of low dimension (2D or 3D for visualization). For instance, two dimensions might be topic and sentiment. Along the topic axis we place a continuum of concepts (maybe ranging from “economy” to “politics” smoothly), and along sentiment from negative to positive. We can simulate short texts that have a defined trajectory in this space (like start at politics-neutral, move to politics-negative as a scandal is described, then move to economy-positive as it mentions a market rally, etc.). These synthetic “texts” can be generated by some procedure that ensures a known ground-truth path in the semantic space.
Model: We then implement two models: (a) a standard Transformer or RNN that reads sequences of tokens (with tokens associated roughly to concepts or changes) – this is the discrete baseline; (b) a Neural ODE model that represents the state in a 2D continuous semantic state and is trained to output the same summary or prediction as the discrete model. The continuous model will learn parameters for diffusion, drift, etc., in this simple space.
Task: One possible task is trajectory prediction: given the first part of a trajectory (text), predict where it will end (e.g., predict the final sentiment or topic – like reading half a news piece and guessing its conclusion). Another could be anomaly detection: detect if a story’s semantic trajectory violates normal patterns (like it jumps erratically – a continuous model should be good at spotting discontinuities).
Evaluation: We measure how well the continuous model captures the ground truth trajectory compared to discrete. For example, we can generate actual continuous trajectories and see if the model’s internal $M(x,t)$ aligns with them (by projecting the model’s discrete representation to concept space for the baseline). We expect the continuous model to represent intermediate states more accurately, as it’s directly optimizing a field.
Metrics: We can measure the smoothness of the learned representation: e.g., check the derivative $dM/dt$ that the model learns, and see if it correlates with meaningful changes (does the model’s notion of instantaneous change line up with actual known moments of change in the text?). We can also measure predictive performance on next token or final outcome, comparing parametric efficiency (e.g., does the continuous model achieve similar accuracy with fewer parameters or layers? Prior work suggests yes
cdn.aaai.org
).
A concrete example for clarity: Suppose we restrict to a vocabulary of a few “concept words” and punctuation that indicates transitions. We might simulate sentences like: “Topic:economy (neutral). [next] downturn (negative). [next] recovery (positive).” And we want the model to output some final summary like “economy ends positive”. The continuous model would treat the input as forcing a path economy→downturn→recovery in the 2D space, which is a loop in topic (stays economy) and a dip then rise in sentiment. The discrete model would see tokens and try to infer that. Over many such examples, the continuous model should learn that “downturn” adds negative sentiment (source in negative sentiment dimension), diffusion might carry some negativity forward until “recovery” injects positive sentiment source. While simplistic, this demonstrates how one can interpret model internals physically. We can then scale up complexity gradually (higher dimension space, more realistic text with embedding mapping to a known semantic axis using tools like LIWC for sentiment or topic categories as coordinates). Furthermore, we would examine if the continuous model can interpolate or handle irregular timing. For instance, feed a sequence with a long pause or irrelevant clause and see if the continuous model naturally handles it by just slow evolution (where a discrete model might struggle with long-range dependency if not enough layers). Neural ODE’s strength is dealing with irregular time steps
math.utah.edu
, which in language terms could equate to skipping some inputs or handling variable lengths smoothly. Another interesting experiment: Meaning Conservation vs. Change. We could enforce a law like conservation of total meaning of certain type (like total “mass” of meaning is constant if it’s just moving around topics without losing info). Does the continuous model learn something akin to conservation when appropriate? We could test this by checking if $\int_x M(x,t) dx$ remains stable for models in contexts where no new info is added (no new tokens, just rephrasing). A good continuous model might keep the integral same (meaning conserved) whereas a naive discrete one might not explicitly maintain that. At a more ambitious level, one could implement a small-scale version of the semantic field model on real text: use a pre-trained embedding space (like a PCA-reduced concept space) as our $x$ domain, and train a neural ODE to map from sentence start to sentence end in that space. For example, take thousands of sentence pairs where one is prefix and one is full sentence, and train the ODE to predict the full sentence embedding from partial sentence. This tests if continuous integration can model how meaning builds up. We might find that an ODE with learned parameters can indeed extrapolate the meaning trajectory more gracefully than a recurrent net with fixed steps.
Theoretical Alignment and Cross-Reference to Paper 1
It’s important to connect this continuous formulation with the discrete Harmonia model (Paper 1) to ensure they form a unified framework. As mentioned, if we discretize the field, we should get something like Harmonia. Indeed:
Harmonia’s token lattice becomes, in the limit, the diffusion graph embedded in our $\nabla_x^2$ term. Harmonia explicitly connected related tokens; in the field, related concepts automatically influence each other by diffusion. One could say Harmonia’s lattice is a sparse approximation of a full diffusion operator on concept space.
Harmonia’s ideogram decomposition provided a basis of sub-concepts for tokens, which is analogous to how we would choose basis functions $\phi_i(x)$ for $M(x,t)$. In fact, the primitives in ideograms (sun, moon, etc.) could serve as coordinates in semantic space. So one can see [Paper 1] as choosing an explicit semantic basis and writing each token as combination of basis elements. [Paper 2] then allows any combination (not just those tied to specific tokens) and continuous interpolation between them.
In [Paper 1], we had hierarchical Transformers to integrate context and lattice relations. In [Paper 2], those roles are taken by continuous integration over time (context) and space (relations). The hierarchical layers become, in effect, the time discretization of an ODE solver. If [Paper 1] used $L$ layers, [Paper 2] would aim to use an integral from $0$ to $L$ continuously. If needed, one can recover a specific layer’s equivalent by sampling $M(x,t)$ at some intermediate $t$.
It is also worth noting that some theoretical constructs in the FIL (Fundamental Interaction Language) theory resonate here:
The FIL kernel was defined in [Paper 1] references as $k(v_1, v_2) = \langle \psi_{v1} | \hat{M} | \psi_{v2} \rangle$, summing specialized components like structural, semantic, temporal similarities. In our field terms, such a kernel could be seen as evaluating overlaps in the semantic field under certain projections. If our semantic field formalism is correct, we should be able to derive similar kernel evaluations by integrating the field against measurement functions (like $\hat{M}$ could be integrating $M(x,t)$ against some context function).
The idea of semantic momentum and semantic positions with a symplectic structure suggests we might treat the pair $(x, p)$ in our system, where $p$ could be like the “rate of change of meaning” (conjugate momentum to the position in semantic space). Indeed, if we consider the space of $(M, \partial M/\partial t)$, one can define something akin to momentum. However, delving into that would bring us into Hamiltonian dynamics of semantics, which is fascinating but beyond current scope. Still, the continuous model could potentially conserve a Hamiltonian (like a quantity for total information or consistency).
There is also the notion of a speed limit for thought ($c_{\text{comp}}$). If meaning propagation obeys a limit, our field $M(x,t)$ might not be allowed to change arbitrarily fast. In practice, a neural ODE’s step size adaptivity or a regularization could enforce such a limit on $|dM/dt|$. This would mean the model can’t change its mind infinitely quickly – which could tie to a physical bound or simply act as a regularizer to prevent overfitting abrupt changes. It’s an interesting area: perhaps we ensure $|\partial M/\partial t| < c_{\text{comp}}$ in training, which could align with the idea that there's an upper bound on how fast information can be processed or meaning can be changed (linked to energy constraints).
In summary, our semantic calculus approach is very much continuous in spirit with the theoretical frameworks being explored contemporaneously. By formalizing language in the language of calculus and physics, we gain not only new descriptive power but also draw closer to fundamental questions about cognition and computation. After all, if intelligence and understanding are physical processes, then modeling them with the appropriate physical-mathematical tools (calculus, differential equations, conservation laws) might be the key to more robust and general AI.
Conclusion: Toward a Unified Theory of Language Dynamics
We have presented a vision for moving NLP beyond discrete sequences into the realm of continuous fields and calculus. This semantic calculus approach complements and builds upon the discrete, graph-based approach of [Paper 1]. Where the first paper (“Regularization of Semantic Embedding Manifolds...”) introduced the idea of structuring and connecting tokens to fix local geometry, this second paper takes the idea to the limit by treating meaning as a continuous quantity. Together, they form a two-part thesis:
Paper 1: Ensure the micro-level consistency of meaning representation (tokens on a manifold) by adding structure (lattices, ideograms).
Paper 2: Develop the macro-level, global view of meaning flow by treating language as a continuous process (a field with integrals and derivatives).
Throughout this paper, we interwove narrative intuitions, mathematical formulation, and references to analogous developments in physics and AI. The narrative example illustrated qualitatively how meaning flows in a story, and the mathematical sections began to pin that down with PDEs and ODE-based models. There is much work ahead to turn this into a practical system, but we can already foresee some transformative implications:
A language model based on semantic calculus could inherently mitigate issues of long-range dependency, because it does not rely on finite unrolling through many layers; instead it integrates as far as needed. This directly addresses the challenge of capturing context over long texts: we can integrate over a long interval without degradation, given a well-parameterized $f$.
The model would provide continuous interpretability. At any fractional time or in-between token, we have a meaningful state (no pun intended) that can be inspected. One could pause the reading at 50% and examine $M(x,0.5)$ to see which concepts are activated – effectively reading the model’s mind mid-stream. This could be useful for interactive or real-time interpretation of model decisions.
By linking with physical law, we might apply physical thinking to NLP. Could we, for example, define an energy of a sentence and say low-energy transformations are paraphrases while high-energy ones change meaning drastically? (Energy ~ something like integral of $(dM/dt)^2$ maybe). Or use ideas of least action: the path a narrative takes might be one that extremizes some action functional (some narratives are more “natural” than others, which could tie to a principle of least cognitive action).
The cross-pollination with physics is not just metaphorical. If meaning fields follow equations analogous to known physical systems, we can import a wealth of analytic tools. Solutions to diffusion equations, resonance in wave equations, phase transitions in reaction-diffusion – these phenomena might correspond to linguistic ones (e.g., abrupt shift in story tone as a phase change). There is also a tantalizing possibility: quantization. If semantic space is like a Hilbert space, one might ask about quantum analogies – indeed entanglement and uncertainty were proposed in the Semantic Hilbert Space formalism. A field theory of language might eventually incorporate a quantum view, but our current focus is the classical continuum (differentiable, not probabilistic superposition except as analogy).
Finally, we reflect on the journey from [Paper 1] to [Paper 2]. In [Paper 1], we often referenced maintaining harmony in the semantic manifold (hence Harmonia). Here, we sought a deeper Harmonia – one between discrete and continuous, between algorithm and calculus, between the knowledge graph and the embedding space. By formally cross-referencing each other (as we have done in the text), these two papers reinforce the point that discrete symbolic structure and continuous fluid semantics are two sides of the same coin. Language has dual nature: like light as particle and wave. The token lattice approach addressed the particle side (distinct pieces of meaning and their relations), and the field calculus addresses the wave side (meaning as a spread, a flow). Embracing both is likely necessary for a full theory of language. In closing, we emphasize that this semantic calculus is not just an academic exercise, but has practical avenues. We outlined a prototype experiment and expect future work to implement these ideas in at least restricted domains. The potential benefits – models that understand context deeply, that are resilient to rephrasing and long texts, that integrate knowledge seamlessly, and that even respect computational limits like energy or speed – make this a promising direction. As the famous quote (slightly adapted) might run: The laws of meaning are not different from the laws of the universe; by learning to calculate meaning, we learn to speak in the language of nature itself. In more concrete terms, by developing a calculus for semantics, we edge closer to AI systems that operate in harmony with fundamental principles, hopefully resulting in more robust, interpretable, and human-aligned intelligence. [Paper 1] has laid the groundwork by ensuring our tokens and their embeddings are well-behaved and knowledge-infused. Building on that foundation, the work presented here in Paper 2 sketches the future where those tokens dissolve into a smooth field – a future where we might integrate an entire book’s meaning and take derivatives of an argument’s logic. The path to that future is challenging, but step by step (or integral by integral), the unification of linguistic structure and semantic continuum becomes achievable. Each equation and each experiment will bring us closer to a Unified Field Theory of Language, fulfilling, in a computational sense, the age-old dream of understanding the essence of communication and thought.